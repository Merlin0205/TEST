{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWyf95hUq9kkmqPrKcqQtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Merlin0205/TEST/blob/main/FINAL_TESTY_LEDEN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Segmentation for a Clothing Company"
      ],
      "metadata": {
        "id": "LHaiXnQ04uhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n"
      ],
      "metadata": {
        "id": "uWV1gQaJ58xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to build an automated customer segmentation system for a fashion business that operates both online and through physical stores with a loyalty program.\n",
        "\n",
        "The system aims to:\n",
        "- Segment customers based on their purchasing behavior, demographics, and preferences.\n",
        "- Connect this data with inventory information to generate personalized product recommendations.\n",
        "\n",
        "### Key Goals\n",
        "1. **Customer Segmentation**  \n",
        "   Group customers into meaningful segments using clustering techniques based on relevant attributes, such as:\n",
        "   - Spending habits\n",
        "   - Purchase frequency\n",
        "   - Product preferences\n",
        "   - Engagement metrics\n",
        "\n",
        "2. **Personalized Product Offers**  \n",
        "   Recommend products tailored to each customer segment by analyzing:\n",
        "   - Customer preferences\n",
        "   - Inventory data (stock availability and profit margins)\n",
        "\n",
        "3. **Inventory Optimization**  \n",
        "   Prioritize promoting products that:\n",
        "   - Have sufficient stock levels\n",
        "   - Provide higher profit margins  \n",
        "   This ensures maximum revenue while maintaining relevance to the target segments.\n",
        "\n",
        "4. **Scalable Design**  \n",
        "   Create a solution that:\n",
        "   - Can scale to handle larger datasets\n",
        "   - Maintains efficiency and adaptability for various marketing **strategies**"
      ],
      "metadata": {
        "id": "T28iT5Ui6RRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate, check and prepare data"
      ],
      "metadata": {
        "id": "mBOHUjog5hxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   K-menas -- dataset name **clustering_dataset_scaled**\n",
        "*   Inventory -- dataset name **inventory_dataset**\n"
      ],
      "metadata": {
        "id": "cLZWVejRCXnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries"
      ],
      "metadata": {
        "id": "oHvg75AiU8dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "!pip install gradio\n",
        "!pip install google-generativeai --upgrade"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Kx-3xOGjVGEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate dataset 1 \"behavioral_dataset\" and \"behavioral_dataset_noEMAIL\""
      ],
      "metadata": {
        "id": "dhjXafyd-fR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Behavioral Dataset provides information about customer purchasing behavior, serving as the foundation for segmentation. It combines transactional data from the e-shop and loyalty program."
      ],
      "metadata": {
        "id": "TrlBLP26wz9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Setting the number of customers\n",
        "num_customers = 2000\n",
        "\n",
        "# Generate Behavioral Dataset\n",
        "behavioral_data = {\n",
        "    \"customer_id\": [i for i in range(1, num_customers + 1)],  # Unique customer IDs\n",
        "    \"email\": [fake.email() for _ in range(num_customers)],  # Realistic email addresses\n",
        "    \"total_spent\": [round(random.uniform(50, 5000), 2) for _ in range(num_customers)],  # Random total spending\n",
        "    \"total_orders\": [random.randint(1, 50) for _ in range(num_customers)],  # Number of orders placed\n",
        "    \"avg_order_value\": [],  # To be calculated based on total_spent and total_orders\n",
        "    \"last_purchase_days_ago\": [random.randint(0, 365) for _ in range(num_customers)],  # Days since last purchase\n",
        "    \"categories_bought\": [random.randint(1, 6) for _ in range(num_customers)],  # Number of unique categories\n",
        "    \"brands_bought\": [random.randint(1, 6) for _ in range(num_customers)],  # Number of unique brands\n",
        "}\n",
        "\n",
        "# Calculate avg_order_value and add errors deliberately\n",
        "for i in range(num_customers):\n",
        "    if i % 50 == 0:  # Every 50th row will have a missing avg_order_value\n",
        "        behavioral_data[\"avg_order_value\"].append(None)\n",
        "    else:\n",
        "        total_orders = behavioral_data[\"total_orders\"][i]\n",
        "        total_spent = behavioral_data[\"total_spent\"][i]\n",
        "        avg_value = total_spent / total_orders if total_orders > 0 else 0\n",
        "        behavioral_data[\"avg_order_value\"].append(round(avg_value, 2))\n",
        "\n",
        "# Introduce specific errors into the dataset\n",
        "for i in range(20):  # Add invalid email addresses for the first 20 customers\n",
        "    behavioral_data[\"email\"][i] = \"invalid_email.com\" if i % 2 == 0 else \"user@@example.com\"\n",
        "\n",
        "for i in range(2):  # Add negative total_spent for 2 customers\n",
        "    behavioral_data[\"total_spent\"][i] = -random.uniform(100, 500)\n",
        "\n",
        "for i in range(num_customers - 70, num_customers):  # Customers with no purchase data\n",
        "    behavioral_data[\"total_spent\"][i] = None\n",
        "    behavioral_data[\"total_orders\"][i] = 0\n",
        "    behavioral_data[\"avg_order_value\"][i] = None\n",
        "    behavioral_data[\"categories_bought\"][i] = None\n",
        "    behavioral_data[\"brands_bought\"][i] = None\n",
        "    behavioral_data[\"last_purchase_days_ago\"][i] = None\n",
        "\n",
        "# Convert to DataFrame\n",
        "behavioral_df = pd.DataFrame(behavioral_data)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "behavioral_df.head()\n",
        "\n",
        "# Save the dataset to a variable for further use\n",
        "behavioral_dataset = behavioral_df\n",
        "behavioral_dataset.sample(20)"
      ],
      "metadata": {
        "id": "lqvd8G8X-cfz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze dataset for iconsitencies"
      ],
      "metadata": {
        "id": "IrERcQfERy8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "import numpy as np\n",
        "\n",
        "# Generate basic descriptive statistics for numeric columns\n",
        "print(\"Descriptive Statistics for Numeric Columns:\")\n",
        "print(behavioral_dataset.describe())\n",
        "\n",
        "# Check for unique values in categorical columns\n",
        "print(\"\\nUnique values in 'email':\")\n",
        "print(behavioral_dataset['email'].nunique(), \"unique email addresses out of\", len(behavioral_dataset))\n",
        "\n",
        "# Identify invalid email formats\n",
        "invalid_emails = behavioral_dataset[~behavioral_dataset['email'].str.contains(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', na=False)]\n",
        "print(\"\\nInvalid email addresses:\")\n",
        "print(invalid_emails)\n",
        "\n",
        "# Check for negative or zero values in total_spent\n",
        "negative_spent = behavioral_dataset[behavioral_dataset['total_spent'] < 0]\n",
        "print(\"\\nRows with negative 'total_spent':\")\n",
        "print(negative_spent)\n",
        "\n",
        "# Check for customers with zero total_orders but non-zero total_spent\n",
        "inconsistent_data = behavioral_dataset[(behavioral_dataset['total_orders'] == 0) & (behavioral_dataset['total_spent'] > 0)]\n",
        "print(\"\\nRows where 'total_orders' == 0 but 'total_spent' > 0:\")\n",
        "print(inconsistent_data)\n",
        "\n",
        "# Analyze 'categories_bought' and 'brands_bought' for unrealistic values\n",
        "print(\"\\nAnalysis of 'categories_bought' and 'brands_bought':\")\n",
        "print(\"Unique values in 'categories_bought':\", behavioral_dataset['categories_bought'].unique())\n",
        "print(\"Unique values in 'brands_bought':\", behavioral_dataset['brands_bought'].unique())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rTMQwKPFR1z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to Remove from the Dataset\n",
        "Based on the analysis of the dataset, the following data should be removed to ensure consistency and accuracy:\n",
        "\n",
        "Rows with invalid email addresses:\n",
        "\n",
        "Rows with emails that do not follow a valid email format (e.g., invalid_email.com, user@@example.com) should be removed, as these cannot be used for communication or further analysis.\n",
        "Rows with negative total_spent:\n",
        "\n",
        "Rows where total_spent is negative should be removed, as negative spending is illogical and likely indicates data entry errors.\n",
        "Rows with missing values:\n",
        "\n",
        "Rows where any critical fields (total_spent, categories_bought, brands_bought) are missing (NaN) should be removed to maintain dataset integrity. These include customers who registered but have no purchase data (e.g., rows with categories_bought and brands_bought as NaN)."
      ],
      "metadata": {
        "id": "OAkPIcsTSowx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with invalid email addresses\n",
        "behavioral_dataset = behavioral_dataset[\n",
        "    behavioral_dataset['email'].str.contains(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', na=False)\n",
        "]\n",
        "\n",
        "# Remove rows with negative total_spent\n",
        "behavioral_dataset = behavioral_dataset[\n",
        "    behavioral_dataset['total_spent'] >= 0\n",
        "]\n",
        "\n",
        "# Remove rows with missing critical values\n",
        "behavioral_dataset = behavioral_dataset.dropna(\n",
        "    subset=['total_spent', 'categories_bought', 'brands_bought']\n",
        ")\n",
        "\n",
        "# Display summary of cleaned dataset\n",
        "print(\"Summary of cleaned dataset:\")\n",
        "print(behavioral_dataset.info())\n",
        "\n",
        "# Display the first few rows of the cleaned dataset\n",
        "behavioral_dataset.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hhg0GTg9Sq2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze AGAIN dataset for iconsitencies if they were renoved"
      ],
      "metadata": {
        "id": "LhPh8TUgTb0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required library\n",
        "import numpy as np\n",
        "\n",
        "# Function to check and report issues in the dataset\n",
        "def check_data_issues(dataset):\n",
        "    issues_found = False  # Flag to track if any issues are found\n",
        "\n",
        "    print(\"=== Dataset Integrity Check ===\")\n",
        "\n",
        "    # Descriptive statistics\n",
        "    print(\"\\n[INFO] Basic Descriptive Statistics:\")\n",
        "    print(dataset.describe())\n",
        "\n",
        "    # Check for unique emails\n",
        "    unique_emails = dataset['email'].nunique()\n",
        "    total_records = len(dataset)\n",
        "    print(f\"\\n[INFO] Unique email addresses: {unique_emails} out of {total_records}\")\n",
        "\n",
        "    # Invalid email formats\n",
        "    invalid_emails = dataset[~dataset['email'].str.contains(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', na=False)]\n",
        "    if not invalid_emails.empty:\n",
        "        issues_found = True\n",
        "        print(\"\\n[WARNING] Invalid email addresses found:\")\n",
        "        print(invalid_emails)\n",
        "    else:\n",
        "        print(\"\\n[INFO] All email addresses are valid.\")\n",
        "\n",
        "    # Negative total_spent\n",
        "    negative_spent = dataset[dataset['total_spent'] < 0]\n",
        "    if not negative_spent.empty:\n",
        "        issues_found = True\n",
        "        print(\"\\n[WARNING] Rows with negative 'total_spent':\")\n",
        "        print(negative_spent)\n",
        "    else:\n",
        "        print(\"\\n[INFO] No negative values in 'total_spent'.\")\n",
        "\n",
        "    # Inconsistencies: zero total_orders with non-zero total_spent\n",
        "    inconsistent_data = dataset[(dataset['total_orders'] == 0) & (dataset['total_spent'] > 0)]\n",
        "    if not inconsistent_data.empty:\n",
        "        issues_found = True\n",
        "        print(\"\\n[WARNING] Rows where 'total_orders' == 0 but 'total_spent' > 0:\")\n",
        "        print(inconsistent_data)\n",
        "    else:\n",
        "        print(\"\\n[INFO] No inconsistencies in 'total_orders' and 'total_spent'.\")\n",
        "\n",
        "    # Check categories_bought and brands_bought for missing values\n",
        "    if dataset['categories_bought'].isnull().any() or dataset['brands_bought'].isnull().any():\n",
        "        issues_found = True\n",
        "        print(\"\\n[WARNING] Missing values found in 'categories_bought' or 'brands_bought'.\")\n",
        "    else:\n",
        "        print(\"\\n[INFO] No missing values in 'categories_bought' or 'brands_bought'.\")\n",
        "\n",
        "    # Final report\n",
        "    if issues_found:\n",
        "        print(\"\\n[RESULT] Issues detected in the dataset. Please review the warnings above.\")\n",
        "    else:\n",
        "        print(\"\\n[RESULT] All data checks passed. Dataset is clean and ready for analysis.\")\n",
        "\n",
        "# Run the function to check the dataset\n",
        "check_data_issues(behavioral_dataset)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Y6tzQplTdIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we remove the email column from the dataset to ensure customer privacy and focus on anonymized data analysis. The resulting dataset, named behavioral_dataset_noEMAIL, retains all other customer information but excludes email addresses. **This step is essential for data protection and compliance with privacy regulations.**"
      ],
      "metadata": {
        "id": "87hHp0_aYvtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataset without the email column\n",
        "behavioral_dataset_noEMAIL = behavioral_dataset.drop(columns=['email'])\n",
        "\n",
        "# Display the first few rows of the new dataset\n",
        "behavioral_dataset_noEMAIL.head(20)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H51BJ7rFWbhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate dataset 2 \"Preference_dataset\""
      ],
      "metadata": {
        "id": "r1bOk5Pbo3Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Preference Dataset focuses on customer purchasing preferences, capturing insights such as favorite product categories, brands, and price ranges. This dataset will provide valuable information for targeted marketing and segmentation."
      ],
      "metadata": {
        "id": "kfCA-B4Xwh9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate dataset"
      ],
      "metadata": {
        "id": "4EQVqW24OJtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Categories and Brands for the clothing and accessories e-shop\n",
        "CATEGORIES = [\"Tops\", \"Bottoms\", \"Dresses\", \"Outerwear\", \"Shoes\", \"Accessories\", \"Sportswear\"]\n",
        "BRANDS = [\n",
        "    \"Nike\", \"Adidas\", \"Puma\", \"Zara\", \"H&M\", \"Gucci\", \"Prada\", \"Levi's\", \"Ralph Lauren\", \"Under Armour\",\n",
        "    \"Calvin Klein\", \"New Balance\", \"Tommy Hilfiger\", \"Versace\", \"Burberry\"\n",
        "]\n",
        "\n",
        "# Number of customers (same as in Behavioral Dataset)\n",
        "num_customers = 2000\n",
        "\n",
        "# Generate Preference Dataset\n",
        "preference_data = {\n",
        "    \"customer_id\": [i for i in range(1, num_customers + 1)],  # Unique customer IDs\n",
        "    \"top_category\": [random.choice(CATEGORIES) for _ in range(num_customers)],  # Most frequent category\n",
        "    \"top_brand\": [random.choice(BRANDS) for _ in range(num_customers)],  # Most frequent brand\n",
        "    \"price_preference_range\": [random.randint(1, 3) for _ in range(num_customers)],  # 1 = Low, 2 = Mid, 3 = High\n",
        "    \"discount_sensitivity\": [round(random.uniform(0.0, 1.0), 2) for _ in range(num_customers)],  # Sensitivity to discounts\n",
        "    \"luxury_preference_score\": [random.randint(1, 5) for _ in range(num_customers)]  # Preference for luxury (1-5)\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "preference_df = pd.DataFrame(preference_data)\n",
        "\n",
        "\n",
        "# Save the dataset to a variable for further use\n",
        "preference_dataset_names = preference_df\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "preference_dataset_names.sample(20)\n"
      ],
      "metadata": {
        "id": "3hofIVrOuLLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code converts categorical features (top category and brand) in the preference dataset to numerical IDs for use in the K-means clustering algorithm, which requires numerical input."
      ],
      "metadata": {
        "id": "V2aZpNIMySMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# Create mapping tables for 'top_category' and 'top_brand'\n",
        "category_mapping = {category: idx for idx, category in enumerate(CATEGORIES)}\n",
        "brand_mapping = {brand: idx for idx, brand in enumerate(BRANDS)}\n",
        "\n",
        "# Save the mapping tables to DataFrames for future use\n",
        "category_mapping_df = pd.DataFrame(list(category_mapping.items()), columns=[\"category_name\", \"category_id\"])\n",
        "brand_mapping_df = pd.DataFrame(list(brand_mapping.items()), columns=[\"brand_name\", \"brand_id\"])\n",
        "\n",
        "# Display the mapping tables as tables\n",
        "print(\"Category Mapping Table:\")\n",
        "display(category_mapping_df)\n",
        "\n",
        "print(\"\\nBrand Mapping Table:\")\n",
        "display(brand_mapping_df)\n",
        "\n",
        "# Convert 'top_category' and 'top_brand' to numeric values in preference_dataset_names\n",
        "preference_dataset = preference_dataset_names.copy()  # Create a copy to store the results\n",
        "preference_dataset[\"top_category\"] = preference_dataset_names[\"top_category\"].map(category_mapping)\n",
        "preference_dataset[\"top_brand\"] = preference_dataset_names[\"top_brand\"].map(brand_mapping)\n",
        "\n",
        "# Display 20 random rows of the updated dataset as a table\n",
        "print(\"\\nUpdated Preference Dataset with Numeric Values (Sample of 20 rows):\")\n",
        "display(preference_dataset.sample(20))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vZS8eXV4ySay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate dataset 3 \"Inventory_dataset\""
      ],
      "metadata": {
        "id": "J6e9BJ_Nt67Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Inventory Dataset contains detailed information about the products available in stock for an e-commerce clothing and accessories store. It is logically connected to the categories and brands used in the Preference Dataset to maintain consistency across datasets. This dataset is essential for identifying products to promote, optimizing stock levels, and calculating profit margins for personalized offers."
      ],
      "metadata": {
        "id": "p3Tk54ZewuTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Categories, Brands, Colors, and Adjectives\n",
        "CATEGORIES = [\"Tops\", \"Bottoms\", \"Dresses\", \"Outerwear\", \"Shoes\", \"Accessories\", \"Sportswear\"]\n",
        "BRANDS = [\n",
        "    \"Nike\", \"Adidas\", \"Puma\", \"Zara\", \"H&M\", \"Gucci\", \"Prada\", \"Levi's\", \"Ralph Lauren\", \"Under Armour\",\n",
        "    \"Calvin Klein\", \"New Balance\", \"Tommy Hilfiger\", \"Versace\", \"Burberry\"\n",
        "]\n",
        "ADJECTIVES = [\"Classic\", \"Modern\", \"Stylish\", \"Luxury\", \"Casual\", \"Comfortable\", \"Premium\"]\n",
        "COLORS = [\"Red\", \"Blue\", \"Black\", \"White\", \"Green\", \"Beige\", \"Pink\", \"Grey\"]\n",
        "\n",
        "# Number of products\n",
        "num_products = 1000\n",
        "\n",
        "# Generate unique product names\n",
        "unique_product_names = set()\n",
        "\n",
        "def generate_unique_product_name():\n",
        "    \"\"\"Generates a unique product name.\"\"\"\n",
        "    while True:\n",
        "        brand = random.choice(BRANDS)\n",
        "        category = random.choice(CATEGORIES)\n",
        "        adjective = random.choice(ADJECTIVES)\n",
        "        color = random.choice(COLORS)\n",
        "        product_name = f\"{brand} {color} {adjective} {category}\"\n",
        "        if product_name not in unique_product_names:\n",
        "            unique_product_names.add(product_name)\n",
        "            return product_name\n",
        "\n",
        "# Generate Inventory Dataset\n",
        "inventory_data = {\n",
        "    \"product_id\": [i for i in range(1, num_products + 1)],\n",
        "    \"product_name\": [generate_unique_product_name() for _ in range(num_products)],\n",
        "    \"category\": [],\n",
        "    \"brand\": [],\n",
        "    \"stock_quantity\": [random.randint(0, 100) for _ in range(num_products)],\n",
        "    \"retail_price\": [round(random.uniform(300, 5000), 2) for _ in range(num_products)],\n",
        "    \"cost_price\": [],\n",
        "    \"profit_margin\": []\n",
        "}\n",
        "\n",
        "# Populate category and brand based on product_name\n",
        "for product_name in inventory_data[\"product_name\"]:\n",
        "    split_name = product_name.split(\" \")\n",
        "    inventory_data[\"brand\"].append(split_name[0])\n",
        "    inventory_data[\"category\"].append(split_name[-1])\n",
        "\n",
        "# Calculate cost_price and profit_margin\n",
        "for i in range(num_products):\n",
        "    retail_price = inventory_data[\"retail_price\"][i]\n",
        "    profit_margin = round(random.uniform(50, 100), 2) / 100\n",
        "    cost_price = round(retail_price * (1 - profit_margin), 2)\n",
        "    inventory_data[\"cost_price\"].append(cost_price)\n",
        "    inventory_data[\"profit_margin\"].append(round(profit_margin * 100, 2))\n",
        "\n",
        "# Convert to DataFrame\n",
        "inventory_df = pd.DataFrame(inventory_data)\n",
        "\n",
        "# Save the dataset to a variable for further use\n",
        "inventory_dataset = inventory_df\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "inventory_dataset.sample(20)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qw4pfpsqo2wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging Preference and Behavioral Datasets"
      ],
      "metadata": {
        "id": "f5pRFOyE3ykw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**combined_dataset_scaled** is the final daset for segmentation\n",
        "Data scaling is essential before applying K-means clustering because the algorithm relies on Euclidean distance to measure the similarity between data points. Features with larger numerical ranges, such as total_spent, will dominate the distance calculations, making features with smaller ranges, like discount_sensitivity, insignificant. By scaling the dataset, all features contribute equally to the clustering process, ensuring balanced and meaningful results."
      ],
      "metadata": {
        "id": "5WasiENY6CfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge preference_dataset and behavioral_dataset_noEMAIL on 'customer_id'\n",
        "combined_dataset = pd.merge(preference_dataset, behavioral_dataset_noEMAIL, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Display 20 random rows from the merged dataset\n",
        "print(\"\\nCombined Dataset (Sample of 20 rows):\")\n",
        "combined_dataset.sample(20)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BrAtZDTo4Anr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares a dataset for clustering by performing several key preprocessing steps. It begins by checking for missing values and removing incomplete rows to ensure data integrity. Next, numeric features are standardized using StandardScaler to ensure all variables contribute equally to the clustering analysis, while preserving the customer_id column to retain customer identity. The final standardized dataset is displayed for verification and further use in clustering algorithms."
      ],
      "metadata": {
        "id": "Fkps8iPvBWvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Check the dataset structure\n",
        "#print(\"Combined Dataset Overview:\")\n",
        "#print(combined_dataset.info())\n",
        "\n",
        "# Step 2: Check for missing values in each column\n",
        "print(\"\\nChecking for missing values in the dataset:\")\n",
        "print(combined_dataset.isnull().sum())\n",
        "\n",
        "# Step 3: Summary statistics to identify outliers or inconsistencies\n",
        "print(\"\\nSummary statistics for numeric columns:\")\n",
        "print(combined_dataset.describe())\n",
        "\n",
        "# Step 4: Handle missing values (keep customer_id intact)\n",
        "clustering_dataset = combined_dataset.dropna()\n",
        "print(\"\\nAfter removing rows with missing values:\")\n",
        "print(clustering_dataset.isnull().sum())\n",
        "\n",
        "# Step 5: Standardize numeric columns to prepare for clustering (ignore customer_id)\n",
        "columns_to_scale = clustering_dataset.drop(columns=[\"customer_id\"]).columns\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(clustering_dataset[columns_to_scale])\n",
        "\n",
        "# Step 6: Combine scaled data with customer_id\n",
        "clustering_dataset_scaled = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
        "clustering_dataset_scaled.insert(0, \"customer_id\", clustering_dataset[\"customer_id\"].values)\n",
        "\n",
        "# Step 7: Display the first 5 rows of the standardized dataset\n",
        "print(\"\\nClustering Dataset after Scaling (Sample of 5 rows):\")\n",
        "clustering_dataset_scaled.sample(20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eiAe5y_K6rCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Ideal Number of Clusters and Perform K-means Analysis"
      ],
      "metadata": {
        "id": "d4BLr-bSDO74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Determine the optimal number of clusters using the Elbow Method and Silhouette Score. Send the results of both analyses to GEMINI for cluster count recommendations.\n",
        "*   Performing the K-means clustering algorithm on the prepared dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nHYav25gEYNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine the optimal number of clusters --- GEMINI - gemini-2.0-flash-exp model"
      ],
      "metadata": {
        "id": "PeyGsaBLEMAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Assume clustering_dataset_scaled is already in memory (Colab environment)\n",
        "# Drop the 'customer_id' column since it is not used for clustering\n",
        "features = clustering_dataset_scaled.drop(columns=[\"customer_id\"]).values\n",
        "\n",
        "# Step 2: Elbow Method to calculate Inertia\n",
        "# Inertia measures how tightly clustered the data points are around the centroids\n",
        "inertia = []\n",
        "K = range(1, 11)  # Check for cluster counts from 1 to 10\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Step 3: Silhouette Score for cluster quality\n",
        "# Silhouette Score measures how similar a point is to its own cluster compared to other clusters\n",
        "silhouette_scores = []\n",
        "k_values = range(2, 11)  # Silhouette Score is undefined for k=1\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(features)\n",
        "    score = silhouette_score(features, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Step 4: Gemini API to Recommend Optimal Clusters\n",
        "# Set your Google API key obtained from the Google Cloud Platform\n",
        "# *** Replace \"YOUR_API_KEY\" with your actual Google Cloud API key ***\n",
        "GOOGLE_API_KEY = 'AIzaSyCKnz_6ISwXYxxc3R2-Bay4ofUg4YXQH54'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def get_optimal_clusters_from_ai(elbow_data, silhouette_data):\n",
        "    \"\"\"\n",
        "    Uses Gemini API to analyze Elbow and Silhouette results and recommend the optimal k.\n",
        "    \"\"\"\n",
        "    # Prompt for the Gemini model\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following clustering data:\n",
        "    Elbow Method Inertia Values: {elbow_data}\n",
        "    Silhouette Scores: {silhouette_data}\n",
        "\n",
        "    Please analyze and recommend the optimal number of clusters (k).\n",
        "    Provide the most appropriate number, but if it is unclear, mention the range of k values like '5-6'.\n",
        "    Use the following format for your answer:\n",
        "    - **Elbow Method**: [number]\n",
        "    - **Silhouette Score**: [number]\n",
        "\n",
        "    Provide a maximum of 2 bullet points explaining each method and finish with one clear concluding sentence.\n",
        "    \"\"\"\n",
        "    # Initialize Gemini model\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "\n",
        "\n",
        "    # Generate content using the Gemini model\n",
        "    # response = model.generate_content(prompt)\n",
        "    response = model.generate_content(prompt, generation_config={\"temperature\": 0.1})\n",
        "    # Return the generated response text\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# Call the API to get the optimal number of clusters\n",
        "ai_response = get_optimal_clusters_from_ai(inertia, silhouette_scores)\n",
        "\n",
        "# Step 5: Visualization Function\n",
        "def generate_visuals(k):\n",
        "    \"\"\"\n",
        "    Generates Elbow and Silhouette Score plots with user-selected k value.\n",
        "    \"\"\"\n",
        "    # Elbow Method Visualization\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.plot(K, inertia, 'bx-', label='Elbow Method')\n",
        "    ax1.axvline(x=k, color='r', linestyle='--', label=f'Selected k={k}')\n",
        "    ax1.set_xlabel('Number of Clusters (k)')\n",
        "    ax1.set_ylabel('Inertia')\n",
        "    ax1.set_title('Elbow Method')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Silhouette Score Visualization\n",
        "    fig2, ax2 = plt.subplots()\n",
        "    ax2.plot(k_values, silhouette_scores, 'bx-', label='Silhouette Score')\n",
        "    ax2.axvline(x=k, color='r', linestyle='--', label=f'Selected k={k}')\n",
        "    ax2.set_xlabel('Number of Clusters (k)')\n",
        "    ax2.set_ylabel('Silhouette Score')\n",
        "    ax2.set_title('Silhouette Score')\n",
        "    ax2.legend()\n",
        "\n",
        "    return fig1, fig2, f\"You selected k={k} as the number of clusters.\\n\\nAI Analysis:\\n{ai_response}\"\n",
        "\n",
        "# Step 6: Gradio Interface for Visualization and AI Recommendation\n",
        "def gradio_interface(k):\n",
        "    fig1, fig2 = generate_visuals(k)\n",
        "    return fig1, fig2\n",
        "\n",
        "# Step 6: Gradio Interface for Visualization and AI Recommendation\n",
        "def gradio_interface(k):\n",
        "    fig1, fig2, message = generate_visuals(k)\n",
        "    return message, fig1, fig2\n",
        "\n",
        "# Global variable to store the slider value\n",
        "selected_k = None\n",
        "\n",
        "# Step 7: Launch Gradio Interface\n",
        "initial_k = 5  # Default initial k to display when Gradio loads\n",
        "\n",
        "with gr.Blocks() as interface:\n",
        "    # AI Explanation Section\n",
        "    gr.Markdown(f\"### AI Analysis Explanation\\n{ai_response}\")\n",
        "\n",
        "    # Row for Graphs\n",
        "    with gr.Row():\n",
        "        elbow_plot = gr.Plot(label=\"Elbow Method\")\n",
        "        silhouette_plot = gr.Plot(label=\"Silhouette Score\")\n",
        "\n",
        "    # Slider Section\n",
        "    with gr.Row():\n",
        "        slider = gr.Slider(2, 10, step=1, label=\"Select Number of Clusters\", value=initial_k)\n",
        "\n",
        "    # Uložit hodnotu ze slideru do globální proměnné\n",
        "    def update_selected_k(k):\n",
        "        global selected_k\n",
        "        selected_k = k\n",
        "        print(f\"Selected value of k: {selected_k}\")  # Kontrola v Colab logu\n",
        "\n",
        "    # Update function for slider\n",
        "    def update_visuals(k):\n",
        "        update_selected_k(k)  # Uloží hodnotu slideru\n",
        "        fig1, fig2, _ = generate_visuals(k)\n",
        "        return fig1, fig2\n",
        "\n",
        "    # Trigger updates on slider change\n",
        "    slider.change(update_visuals, inputs=[slider], outputs=[elbow_plot, silhouette_plot])\n",
        "\n",
        "# Launch Gradio\n",
        "interface.launch(prevent_thread_lock=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kYWaYyI-BOUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected number of clusters is stored in the variable selected_k."
      ],
      "metadata": {
        "id": "jcXLpppGFyWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Selected value of k: {selected_k}\")"
      ],
      "metadata": {
        "id": "2Neo6xV3FgPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering K-means"
      ],
      "metadata": {
        "id": "Z8wmOkztGbTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs K-means clustering on the customer dataset to group customers into distinct segments. It first determines the optimal number of clusters using the Elbow and Silhouette methods, potentially with AI assistance, and then applies the K-means algorithm to assign each customer to a cluster"
      ],
      "metadata": {
        "id": "RQA-STZYHxoo"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset and drop the 'customer_id' column\n",
        "data = clustering_dataset_scaled.drop(columns=['customer_id'])\n",
        "\n",
        "# Get the number of clusters from the selected_k variable\n",
        "n_clusters = selected_k\n",
        "\n",
        "# Initialize the KMeans model\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "# Fit the model to the data\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Get the cluster labels for each data point\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Create a copy of the original dataset to avoid overwriting\n",
        "clustered_data = clustering_dataset_scaled.copy()\n",
        "\n",
        "# Add the cluster labels to the new dataset\n",
        "clustered_data['cluster'] = labels\n",
        "\n",
        "# Display the first few rows of the new dataset with cluster assignments\n",
        "clustered_data.head()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ey1bCw5DHWH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create a new dataset **FINAL_DATASET** where the scaled values used for k-means clustering\n",
        "are transformed back to their original values for easier interpretation\n",
        "and cluster naming."
      ],
      "metadata": {
        "id": "oqxwQyeOZ-XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the FINAL_CLUSTERED dataset with customer_id and cluster\n",
        "FINAL_CLUSTERED = clustered_data[['customer_id', 'cluster']]\n",
        "\n",
        "# Performing LEFT JOIN with behavioral_dataset_noEMAIL, using 'customer_id' for both\n",
        "FINAL_DATASET = pd.merge(\n",
        "    FINAL_CLUSTERED,\n",
        "    behavioral_dataset_noEMAIL,\n",
        "    left_on='customer_id',\n",
        "    right_on='customer_id',  # Explicitly joining on 'customer_id' in both DataFrames\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Performing LEFT JOIN with preference_dataset_names, using 'customer_id' for both\n",
        "FINAL_DATASET = pd.merge(\n",
        "    FINAL_DATASET,\n",
        "    preference_dataset_names,\n",
        "    left_on='customer_id',\n",
        "    right_on='customer_id',  # Explicitly joining on 'customer_id' in both DataFrames\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Displaying the first 5 rows of the resulting dataset\n",
        "FINAL_DATASET.head()\n"
      ],
      "metadata": {
        "id": "a3K-Tz_CPl9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI ANALYSIS - This code leverages Google's Gemini AI to generate descriptive names and characteristics for each customer cluster identified through K-means clustering. It prepares a prompt describing the customer features and data structure, then sends it to the AI for analysis and name suggestions."
      ],
      "metadata": {
        "id": "0O8zktecd4A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "\n",
        "# Configure Gemini AI\n",
        "GOOGLE_API_KEY = 'AIzaSyCKnz_6ISwXYxxc3R2-Bay4ofUg4YXQH54'  # Load the API key from stored secrets\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Prompt template for AI\n",
        "prompt_template = \"\"\"\n",
        "You are an expert in customer behavior analysis and clustering.\n",
        "We have performed K-means clustering on customer data, and the dataset contains the following features for each customer:\n",
        "\n",
        "- **Total spent**: Total amount spent by the customer.\n",
        "- **Total orders**: Total number of orders placed by the customer.\n",
        "- **Average order value**: Average amount spent per order.\n",
        "- **Last purchase days ago**: Number of days since the customer's last purchase.\n",
        "- **Categories bought**: Number of unique product categories purchased.\n",
        "- **Brands bought**: Number of unique brands purchased.\n",
        "- **Top category**: Most frequently purchased product category (e.g., Dresses, Sportswear).\n",
        "- **Top brand**: Most frequently purchased brand (e.g., Levi's, Zara).\n",
        "- **Price preference range**:\n",
        "  - 1 = Low (up to 500 CZK)\n",
        "  - 2 = Mid (501–2000 CZK)\n",
        "  - 3 = High (>2000 CZK)\n",
        "- **Discount sensitivity**: A value between 0–1 indicating how often the customer buys discounted products.\n",
        "- **Luxury preference score**: A score from 1–5 indicating preference for premium products.\n",
        "\n",
        "**Objective**: Propose descriptive names for each cluster based on customer behavior and preferences. Provide the output in the following structured format:\n",
        "Cluster [ID]: [Cluster Name]\n",
        "- Key Characteristics:\n",
        "  - [Characteristic 1]\n",
        "  - [Characteristic 2]\n",
        "  - ...\n",
        "\"\"\"\n",
        "\n",
        "# Generate the prompt dynamically based on cluster data\n",
        "def generate_prompt(data):\n",
        "    cluster_summary = []\n",
        "    clusters = data[\"cluster\"].unique()\n",
        "\n",
        "    for cluster in clusters:\n",
        "        cluster_data = data[data[\"cluster\"] == cluster]\n",
        "\n",
        "        # Calculate average values for numeric columns\n",
        "        avg_values = cluster_data[\n",
        "            [\"total_spent\", \"total_orders\", \"avg_order_value\", \"last_purchase_days_ago\",\n",
        "             \"categories_bought\", \"brands_bought\", \"price_preference_range\",\n",
        "             \"discount_sensitivity\", \"luxury_preference_score\"]\n",
        "        ].mean()\n",
        "\n",
        "        # Get dominant values for categorical columns\n",
        "        dominant_values = {\n",
        "            \"top_category\": cluster_data[\"top_category\"].mode()[0] if not cluster_data[\"top_category\"].mode().empty else None,\n",
        "            \"top_brand\": cluster_data[\"top_brand\"].mode()[0] if not cluster_data[\"top_brand\"].mode().empty else None\n",
        "        }\n",
        "\n",
        "        # Combine numeric and categorical aggregates\n",
        "        summary = {**avg_values.to_dict(), **dominant_values}\n",
        "\n",
        "        # Create cluster summary\n",
        "        cluster_summary.append(\n",
        "            f\"Cluster {cluster}:\\n- Values: {summary}\"\n",
        "        )\n",
        "\n",
        "    return prompt_template + \"\\n\" + \"\\n\".join(cluster_summary)\n",
        "\n",
        "# Call Gemini API for cluster naming\n",
        "def call_gemini_api(prompt):\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "    response = model.generate_content(prompt, generation_config={\"temperature\": 0.2})\n",
        "    return response.text\n",
        "\n",
        "# Extract cluster names and descriptions from AI response\n",
        "def parse_ai_response(response):\n",
        "    cluster_names = {}\n",
        "    cluster_descriptions = {}\n",
        "\n",
        "    # Match structured format for cluster information\n",
        "    clusters = re.findall(r\"Cluster (\\d+): (.*?)\\n- Key Characteristics:\\n((?:  - .*?\\n)+)\", response)\n",
        "    for cluster_id, cluster_name, cluster_description in clusters:\n",
        "        cluster_names[int(cluster_id)] = cluster_name\n",
        "        cluster_descriptions[int(cluster_id)] = cluster_description.strip()\n",
        "\n",
        "    return cluster_names, cluster_descriptions\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    global FINAL_DATASET_AI_CLUSTER_NAMES  # Ensure the dataset is globally accessible\n",
        "\n",
        "    # Assume FINAL_DATASET is already loaded in Colab\n",
        "    if 'FINAL_DATASET' not in globals():\n",
        "        raise ValueError(\"FINAL_DATASET is not loaded. Please ensure the dataset is available.\")\n",
        "\n",
        "    # Generate the prompt\n",
        "    prompt = generate_prompt(FINAL_DATASET)\n",
        "    print(\"\\nGenerated Prompt for AI:\\n\")\n",
        "    print(prompt)\n",
        "\n",
        "    # Call Gemini API and get the response\n",
        "    print(\"\\nAI Response:\\n\")\n",
        "    ai_response = call_gemini_api(prompt)\n",
        "    print(ai_response)\n",
        "\n",
        "    # Parse the AI response\n",
        "    cluster_names, cluster_descriptions = parse_ai_response(ai_response)\n",
        "\n",
        "    # Create a new dataset with added columns\n",
        "    FINAL_DATASET_AI_CLUSTER_NAMES = FINAL_DATASET.copy()\n",
        "    FINAL_DATASET_AI_CLUSTER_NAMES[\"cluster_name\"] = FINAL_DATASET[\"cluster\"].map(cluster_names)\n",
        "    FINAL_DATASET_AI_CLUSTER_NAMES[\"cluster_description\"] = FINAL_DATASET[\"cluster\"].map(cluster_descriptions)\n",
        "\n",
        "    # Display the updated dataset\n",
        "    print(\"\\nUpdated Dataset with Cluster Names and Descriptions (First 10 Rows):\\n\")\n",
        "    display(FINAL_DATASET_AI_CLUSTER_NAMES.head(10))\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ITmDxjn2d6PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analyse if the segmentation is correct"
      ],
      "metadata": {
        "id": "59ONoSFDIyAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Visualization (2D Cluster Plot)\n",
        "What to look for:\n",
        "Well-separated clusters indicate high segmentation quality.\n",
        "Overlapping clusters suggest potential issues with the segmentation logic or the number of clusters."
      ],
      "metadata": {
        "id": "nDJ0XkFiPVud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def visualize_clusters(data):\n",
        "    # Select numeric columns for PCA\n",
        "    numeric_columns = [\n",
        "        \"total_spent\", \"total_orders\", \"avg_order_value\",\n",
        "        \"last_purchase_days_ago\", \"categories_bought\", \"brands_bought\",\n",
        "        \"price_preference_range\", \"discount_sensitivity\", \"luxury_preference_score\"\n",
        "    ]\n",
        "\n",
        "    # Perform PCA to reduce to 2D\n",
        "    pca = PCA(n_components=2)\n",
        "    data[\"pca_1\"] = pca.fit_transform(data[numeric_columns])[:, 0]\n",
        "    data[\"pca_2\"] = pca.fit_transform(data[numeric_columns])[:, 1]\n",
        "\n",
        "    # Plot the clusters\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        x=\"pca_1\", y=\"pca_2\", hue=\"cluster_name\",\n",
        "        data=data, palette=\"Set2\", s=100, alpha=0.8\n",
        "    )\n",
        "\n",
        "    # Add cluster names as text annotations\n",
        "    for cluster, row in data.groupby(\"cluster_name\"):\n",
        "        plt.text(\n",
        "            row[\"pca_1\"].mean(), row[\"pca_2\"].mean(),\n",
        "            cluster, fontsize=10, weight=\"bold\", ha=\"center\", va=\"center\",\n",
        "            bbox=dict(facecolor=\"white\", alpha=0.8, boxstyle=\"round,pad=0.3\")\n",
        "        )\n",
        "\n",
        "    # Improved axis labels and title\n",
        "    plt.title(\"Customer Segments (PCA Visualization)\", fontsize=16)\n",
        "    plt.xlabel(\"Principal Component 1 (Explains Most Variance)\", fontsize=12)\n",
        "    plt.ylabel(\"Principal Component 2 (Secondary Variance Explanation)\", fontsize=12)\n",
        "    plt.legend(title=\"Cluster Name\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "visualize_clusters(FINAL_DATASET_AI_CLUSTER_NAMES)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ly8fVf3aIyP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score\n",
        "What to look for:\n",
        "A value close to 1 means clusters are well separated and cohesive.\n",
        "A value close to 0 means clusters are overlapping.\n",
        "A negative value indicates poorly assigned points."
      ],
      "metadata": {
        "id": "t_pPXN_nPZS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Silhouette Score Calculation\n",
        "numeric_columns = [\n",
        "    \"total_spent\", \"total_orders\", \"avg_order_value\",\n",
        "    \"last_purchase_days_ago\", \"categories_bought\", \"brands_bought\",\n",
        "    \"price_preference_range\", \"discount_sensitivity\", \"luxury_preference_score\"\n",
        "]\n",
        "\n",
        "silhouette_avg = silhouette_score(FINAL_DATASET_AI_CLUSTER_NAMES[numeric_columns],\n",
        "                                  FINAL_DATASET_AI_CLUSTER_NAMES[\"cluster\"])\n",
        "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n"
      ],
      "metadata": {
        "id": "rhUs2MC6Pc70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster Feature Heatmap\n",
        "What to look for:\n",
        "The heatmap shows average values of features for each cluster.\n",
        "Significant differences between clusters indicate well-defined segmentation."
      ],
      "metadata": {
        "id": "Plfu_iiuQBt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate average values for each cluster\n",
        "cluster_means = FINAL_DATASET_AI_CLUSTER_NAMES.groupby(\"cluster_name\")[numeric_columns].mean()\n",
        "\n",
        "# Heatmap Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(cluster_means, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Cluster Averages Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Gp21NoOxQDSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inter-Cluster Distance Map\n",
        "What to look for:\n",
        "A distance matrix shows how far apart clusters are from each other.\n",
        "Well-separated clusters will have larger distances."
      ],
      "metadata": {
        "id": "s39pwoQaQl1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# Calculate cluster centroids\n",
        "centroids = FINAL_DATASET_AI_CLUSTER_NAMES.groupby(\"cluster\")[numeric_columns].mean()\n",
        "\n",
        "# Compute pairwise distances between centroids\n",
        "distance_matrix = cdist(centroids, centroids, metric=\"euclidean\")\n",
        "\n",
        "# Display Distance Matrix as Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(distance_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", xticklabels=centroids.index, yticklabels=centroids.index)\n",
        "plt.title(\"Inter-Cluster Distance Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rrKk6YYOQaOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box Plot for Feature Distributions\n",
        "What to look for:\n",
        "Box plots show how feature values are distributed across clusters.\n",
        "Clearly different distributions confirm meaningful segmentation."
      ],
      "metadata": {
        "id": "fbLnj_efQr_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Boxplot for Total Spent by Cluster\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x=\"cluster_name\", y=\"total_spent\", data=FINAL_DATASET_AI_CLUSTER_NAMES)\n",
        "plt.title(\"Distribution of Total Spent by Cluster\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "afyNM8kdQsyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identifying Target Customers and Creating Customized Email Marketing"
      ],
      "metadata": {
        "id": "PdKhN3RYTsS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP 1 - SELECT PRODUCTS FROM STOCK"
      ],
      "metadata": {
        "id": "jWEOaWVdpL3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code allows users to filter the inventory dataset by stock quantity and profit margin, displaying the top products in an interactive table. Users can manually select product IDs for promotion by entering them into a textbox and saving them with a button. The selected product IDs are stored in a global dataset(selected_items) for use in subsequent steps like customer targeting or email campaigns."
      ],
      "metadata": {
        "id": "H_cBFcYQTy9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize a global variable to store selected items as a dataset\n",
        "selected_items = pd.DataFrame()\n",
        "\n",
        "# Function to filter inventory\n",
        "def filter_inventory(filter_stock, filter_profit, num_products):\n",
        "    \"\"\"\n",
        "    Filter Inventory based on stock and/or profit margin and store as a dataset.\n",
        "    \"\"\"\n",
        "    # Copy the dataset to avoid modifying the original\n",
        "    filtered_data = inventory_dataset.copy()\n",
        "\n",
        "    # Apply filters\n",
        "    if filter_stock:\n",
        "        filtered_data = filtered_data.nlargest(num_products, \"stock_quantity\")\n",
        "    if filter_profit:\n",
        "        filtered_data = filtered_data.nlargest(num_products, \"profit_margin\")\n",
        "\n",
        "    # Reset index for clean output\n",
        "    filtered_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Store the filtered dataset globally\n",
        "    global selected_items\n",
        "    selected_items = filtered_data\n",
        "\n",
        "    # Return filtered results as a DataFrame for display\n",
        "    return filtered_data[[\"product_id\", \"product_name\", \"category\", \"brand\",\n",
        "                          \"stock_quantity\", \"profit_margin\", \"retail_price\", \"cost_price\"]]\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_interface():\n",
        "    \"\"\"\n",
        "    Gradio interface for interactive inventory filtering and selection.\n",
        "    \"\"\"\n",
        "    with gr.Blocks() as demo:\n",
        "        # Title\n",
        "        gr.Markdown(\"## Inventory Analysis and Selection for Promotion\")\n",
        "\n",
        "        # Row for filters\n",
        "        with gr.Row():\n",
        "            filter_stock = gr.Checkbox(label=\"Filter Top by Stock Quantity\", value=True)\n",
        "            filter_profit = gr.Checkbox(label=\"Filter Top by Profit Margin\", value=False)\n",
        "            num_products = gr.Slider(1, 10, step=1, label=\"Number of Products\", value=10)\n",
        "\n",
        "        # Table for displaying filtered results\n",
        "        filtered_table = gr.DataFrame(label=\"Filtered Products\",\n",
        "                                      headers=[\"Product ID\", \"Product Name\", \"Category\", \"Brand\",\n",
        "                                               \"Stock Quantity\", \"Profit Margin (%)\",\n",
        "                                               \"Retail Price\", \"Cost Price\"])\n",
        "\n",
        "        # Button to trigger filtering\n",
        "        filter_button = gr.Button(\"Apply Filters\")\n",
        "\n",
        "        # Functionality: Apply filters and display results\n",
        "        def filter_and_display(filter_stock, filter_profit, num_products):\n",
        "            filtered_results = filter_inventory(filter_stock, filter_profit, num_products)\n",
        "            return filtered_results\n",
        "\n",
        "        # Connect the filter button to the filtering function\n",
        "        filter_button.click(filter_and_display,\n",
        "                            inputs=[filter_stock, filter_profit, num_products],\n",
        "                            outputs=[filtered_table])\n",
        "\n",
        "    # Launch Gradio interface\n",
        "    demo.launch()\n",
        "\n",
        "# Launch the Gradio interface\n",
        "gradio_interface()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C7RWU6p5Tx_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check selected items in selected_items dataset"
      ],
      "metadata": {
        "id": "vxTzyCNgcLgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_items.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5gX-oeg-awkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP 2 - AI TO LINK CUSTOMERS BASED ON SEGMENTATION WITH SLECTED PRODUCTS"
      ],
      "metadata": {
        "id": "n7beQQSupi8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI LIMIT ONLY 100 RECORDS This code identifies customers for selected products using Gemini AI. It dynamically generates a prompt that combines product details (including product name, brand, category, stock quantity, and profit margin) from selected_items and customer segmentation data (e.g., cluster names, preferences, and behavioral metrics) from FINAL_DATASET_AI_CLUSTER_NAMES. The AI analyzes this information and returns a ranked list of the top 5 customers per product, along with reasons for their selection. The results, including product details, customer IDs, brands, categories, and AI-provided reasons, are stored in the selected_customers_for_selected_items dataset for further analysis and use..`"
      ],
      "metadata": {
        "id": "jlfsFco0fBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "\n",
        "# Configure Gemini AI\n",
        "GOOGLE_API_KEY = 'AIzaSyCKnz_6ISwXYxxc3R2-Bay4ofUg4YXQH54'  # Load API key from environment\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Ensure required datasets are loaded\n",
        "if 'selected_items' not in globals() or 'FINAL_DATASET_AI_CLUSTER_NAMES' not in globals():\n",
        "    raise ValueError(\"Make sure `selected_items` and `FINAL_DATASET_AI_CLUSTER_NAMES` datasets are available.\")\n",
        "\n",
        "# Limit the data to 100 random rows\n",
        "filtered_customers = FINAL_DATASET_AI_CLUSTER_NAMES.sample(n=100, random_state=42)\n",
        "\n",
        "# Updated Prompt Template\n",
        "prompt_template = \"\"\"\n",
        "You are an expert in customer segmentation and personalized marketing.\n",
        "\n",
        "We have the following customer data:\n",
        "- Cluster Name and Description: High-level segmentation of customers.\n",
        "- Top Category and Top Brand: Preferences for product categories and brands.\n",
        "- Behavioral Metrics: Total spent, discount sensitivity, luxury preference score.\n",
        "\n",
        "We also have the following products to promote:\n",
        "- Category and Brand: The type of product.\n",
        "- Stock Quantity and Profit Margin: Inventory details.\n",
        "\n",
        "Your task:\n",
        "1. Match customers to products based on category and brand preferences.\n",
        "2. Rank customers based on likelihood to purchase, using behavioral metrics.\n",
        "3. For each product, recommend the top 5 customers and provide reasons for each recommendation.\n",
        "\n",
        "Your response should follow this exact structured format for each product:\n",
        "\n",
        "Product [Product Name], Product ID [Product ID]:\n",
        "- Customer ID [Customer ID]: [Reason for selection]\n",
        "\"\"\"\n",
        "\n",
        "# Generate the AI prompt dynamically\n",
        "def generate_prompt(selected_items, filtered_customers):\n",
        "    product_details = []\n",
        "    for _, product in selected_items.iterrows():\n",
        "        details = f\"Product {product['product_name']}, Product ID {product['product_id']} (Category: {product['category']}, Brand: {product['brand']}, \" \\\n",
        "                  f\"Stock Quantity: {product['stock_quantity']}, Profit Margin: {product['profit_margin']}%)\"\n",
        "        product_details.append(details)\n",
        "\n",
        "    customer_data = filtered_customers[[\n",
        "        \"customer_id\", \"cluster_name\", \"cluster_description\",\n",
        "        \"top_category\", \"top_brand\", \"total_spent\",\n",
        "        \"discount_sensitivity\", \"luxury_preference_score\"\n",
        "    ]].to_dict(orient=\"records\")\n",
        "\n",
        "    prompt = prompt_template + \"\\n\\nProducts:\\n\" + \"\\n\".join(product_details) + \"\\n\\nCustomers:\\n\" + str(customer_data)\n",
        "    return prompt\n",
        "\n",
        "# Call Gemini AI to recommend customers\n",
        "def call_gemini_ai(prompt):\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "    response = model.generate_content(prompt, generation_config={\"temperature\": 0.3})\n",
        "    return response.text\n",
        "\n",
        "# Parse AI response into a structured format\n",
        "def parse_ai_response(response):\n",
        "    recommendations = []\n",
        "    # Najdi všechny produkty\n",
        "    product_matches = re.findall(r\"Product (.*?), Product ID (\\d+)\", response)\n",
        "\n",
        "    for product_name, product_id in product_matches:\n",
        "        # Najdi sekci pro daný produkt\n",
        "        product_section = re.search(\n",
        "            rf\"Product {re.escape(product_name)}, Product ID {product_id}.*?:(.*?)(?=Product|$)\",\n",
        "            response, re.DOTALL\n",
        "        )\n",
        "\n",
        "        if product_section:\n",
        "            # Najdi všechny zákazníky a jejich důvody\n",
        "            customer_lines = re.findall(r\"- Customer ID (\\d+): (.*?)\\n\", product_section.group(1))\n",
        "            for customer_id, reason in customer_lines:\n",
        "                # Najdi detaily produktu v `selected_items`\n",
        "                product_details = selected_items[selected_items[\"product_id\"] == int(product_id)].iloc[0]\n",
        "                recommendations.append({\n",
        "                    \"product_name\": product_name,\n",
        "                    \"product_id\": int(product_id),\n",
        "                    \"customer_id\": int(customer_id),\n",
        "                    \"reason\": reason.strip(),\n",
        "                    \"brand\": product_details[\"brand\"],\n",
        "                    \"category\": product_details[\"category\"]\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(recommendations)\n",
        "\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Generate AI prompt\n",
        "    print(\"Generating prompt for AI...\")\n",
        "    prompt = generate_prompt(selected_items, filtered_customers)\n",
        "    print(\"\\nGenerated Prompt:\\n\", prompt)\n",
        "\n",
        "    # Call Gemini AI and get recommendations\n",
        "    print(\"\\nCalling Gemini AI...\")\n",
        "    ai_response = call_gemini_ai(prompt)\n",
        "    print(\"\\nAI Response:\\n\", ai_response)\n",
        "\n",
        "    # Parse the AI response into a structured dataset\n",
        "    print(\"\\nParsing AI Response...\")\n",
        "    global selected_customers_for_selected_items\n",
        "    selected_customers_for_selected_items = parse_ai_response(ai_response)\n",
        "\n",
        "    # Display the dataset\n",
        "    if not selected_customers_for_selected_items.empty:\n",
        "        print(\"\\nRecommended Customers for Selected Products (First 10 Rows):\")\n",
        "        print(selected_customers_for_selected_items.head(10))\n",
        "    else:\n",
        "        print(\"\\nNo recommendations could be parsed from the AI response.\")\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g-Bu1uYifDkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_customers_for_selected_items.head(20\n",
        "                                           )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d7BeDRwggg3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP 3 - Generate Personalized Marketing Emails TEXT for Selected Products"
      ],
      "metadata": {
        "id": "6mr9_9SFp6mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Personalized Marketing Emails for Selected Products\n",
        "This code dynamically generates personalized email campaigns for customers based on selected products. It uses Gemini AI to create engaging email templates (including subject lines and HTML bodies) for each product, emphasizing a 20% discount and the product's features. The emails are customized for each customer by replacing placeholders with their ID and stored in the selected_customers_for_selected_items dataset with new columns subject and body. This dataset can be used for further actions, such as sending the emails."
      ],
      "metadata": {
        "id": "KDFWrQcZlskY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini AI\n",
        "GOOGLE_API_KEY = 'AIzaSyCKnz_6ISwXYxxc3R2-Bay4ofUg4YXQH54'  # Load API key from environment\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Ensure required datasets are loaded\n",
        "if 'selected_customers_for_selected_items' not in globals() or 'inventory_dataset' not in globals():\n",
        "    raise ValueError(\"Ensure `selected_customers_for_selected_items` and `inventory_dataset` datasets are available.\")\n",
        "\n",
        "# Function to generate email prompt\n",
        "def generate_email_prompt(product_name, category, brand, retail_price, discounted_price):\n",
        "    return f\"\"\"\n",
        "You are an expert email marketing copywriter. Create an engaging email in HTML format to promote a product.\n",
        "\n",
        "Product Details:\n",
        "- Name: {product_name}\n",
        "- Category: {category}\n",
        "- Brand: {brand}\n",
        "- Original Price: {retail_price} CZK\n",
        "- Discounted Price: {discounted_price} CZK\n",
        "\n",
        "Requirements:\n",
        "1. Write a subject line for the email that grabs attention.\n",
        "2. Write an HTML body with the following sections:\n",
        "   - A warm greeting for the customer.\n",
        "   - Highlight the 20% discount and display the original and discounted prices clearly.\n",
        "   - Emphasize the product's features and appeal (name, category, brand).\n",
        "   - Include a call-to-action to \"Buy Now\" with a placeholder link (#).\n",
        "\n",
        "Ensure the email is visually appealing and professional.\n",
        "\"\"\"\n",
        "\n",
        "# Call Gemini AI to generate the email\n",
        "def call_gemini_ai(prompt):\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "    response = model.generate_content(prompt, generation_config={\"temperature\": 0.3})\n",
        "    return response.text\n",
        "\n",
        "# Main function to generate emails\n",
        "def main():\n",
        "    # Extract unique products from the dataset\n",
        "    unique_products = selected_customers_for_selected_items[[\"product_id\", \"product_name\", \"category\", \"brand\"]].drop_duplicates()\n",
        "\n",
        "    # Prepare a mapping of product_id to email templates\n",
        "    email_templates = {}\n",
        "\n",
        "    for _, product in unique_products.iterrows():\n",
        "        product_id = product[\"product_id\"]\n",
        "        product_name = product[\"product_name\"]\n",
        "        category = product[\"category\"]\n",
        "        brand = product[\"brand\"]\n",
        "\n",
        "        # Get retail price and calculate discounted price\n",
        "        retail_price = inventory_dataset[inventory_dataset[\"product_id\"] == product_id][\"retail_price\"].iloc[0]\n",
        "        discounted_price = round(retail_price * 0.8, 2)\n",
        "\n",
        "        # Generate the AI prompt\n",
        "        prompt = generate_email_prompt(product_name, category, brand, retail_price, discounted_price)\n",
        "\n",
        "        # Call Gemini AI\n",
        "        print(f\"Generating email for product: {product_name}...\")\n",
        "        ai_response = call_gemini_ai(prompt)\n",
        "\n",
        "        # Parse AI response\n",
        "        subject_match = re.search(r\"Subject:\\s*(.*)\", ai_response)\n",
        "        body_match = re.search(r\"<html>.*</html>\", ai_response, re.DOTALL)\n",
        "\n",
        "        subject = subject_match.group(1).strip() if subject_match else \"Special Offer for You!\"\n",
        "        body = body_match.group(0).strip() if body_match else ai_response.strip()\n",
        "\n",
        "        email_templates[product_id] = {\"subject\": subject, \"body\": body}\n",
        "\n",
        "    # Update the dataset with personalized emails\n",
        "    def create_email_row(row):\n",
        "        product_id = row[\"product_id\"]\n",
        "        customer_id = row[\"customer_id\"]\n",
        "        email_template = email_templates.get(product_id, {})\n",
        "        subject = email_template.get(\"subject\", \"\")\n",
        "        body = email_template.get(\"body\", \"\").replace(\"{{customer_id}}\", str(customer_id))\n",
        "        return pd.Series([subject, body])\n",
        "\n",
        "    selected_customers_for_selected_items[[\"subject\", \"body\"]] = selected_customers_for_selected_items.apply(create_email_row, axis=1)\n",
        "\n",
        "    # Display the updated dataset\n",
        "    print(\"\\nUpdated Dataset with Marketing Emails (First 10 Rows):\\n\")\n",
        "    display(selected_customers_for_selected_items.head(30))\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-C3uc5URlvxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP 4 - SEND EMAILS TO CUSTOMERS"
      ],
      "metadata": {
        "id": "EhapwRE0qAKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send Unique Marketing Emails\n",
        "This code sends one email for each unique product in the selected_customers_for_selected_items dataset. The emails, formatted in HTML, are sent from romecapstone@gmail.com to jan.krejci@krejca.eu with content dynamically extracted from the dataset. It ensures no duplicate emails are sent for multiple customers linked to the same product."
      ],
      "metadata": {
        "id": "Fh3izCflnMyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "# Gmail SMTP server configuration\n",
        "smtp_server = \"smtp.gmail.com\"\n",
        "port = 587\n",
        "sender_email = \"romecapstone@gmail.com\"  # Replace with your email address\n",
        "password = \"wywx derq duox gndi\"  # Replace with your App Password\n",
        "\n",
        "# Ensure the dataset is loaded\n",
        "if 'selected_customers_for_selected_items' not in globals():\n",
        "    raise ValueError(\"Ensure `selected_customers_for_selected_items` dataset is available.\")\n",
        "\n",
        "# Extract unique products from the dataset\n",
        "unique_products = selected_customers_for_selected_items.drop_duplicates(subset=[\"product_id\"])\n",
        "\n",
        "# Function to send an email\n",
        "def send_email(subject, body, receiver_email):\n",
        "    try:\n",
        "        # Create the email message\n",
        "        msg = MIMEMultipart()\n",
        "        msg[\"From\"] = sender_email\n",
        "        msg[\"To\"] = receiver_email\n",
        "        msg[\"Subject\"] = subject\n",
        "        msg.attach(MIMEText(body, \"html\"))\n",
        "\n",
        "        # Connect to the SMTP server and send the email\n",
        "        with smtplib.SMTP(smtp_server, port) as server:\n",
        "            server.starttls()  # Enable encrypted connection\n",
        "            server.login(sender_email, password)\n",
        "            server.sendmail(sender_email, receiver_email, msg.as_string())\n",
        "\n",
        "        print(f\"Email sent successfully to {receiver_email} for subject: {subject}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending email: {e}\")\n",
        "\n",
        "# Iterate over unique products and send emails\n",
        "receiver_email = \"jan.krejci@krejca.eu\"\n",
        "\n",
        "for _, product in unique_products.iterrows():\n",
        "    subject = product[\"subject\"]\n",
        "    body = product[\"body\"]\n",
        "\n",
        "    print(f\"Preparing to send email for product: {product['product_name']}...\")\n",
        "    send_email(subject, body, receiver_email)\n"
      ],
      "metadata": {
        "id": "CXjNZaVInOJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STREAMIT TEST"
      ],
      "metadata": {
        "id": "oYUglOqUo4Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalace Streamlit a ngrok\n",
        "!pip install streamlit pyngrok --quiet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbi6QXp-o7IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Moje Aplikace v Colabu\")\n",
        "st.write(\"Toto je ukázkový příklad, jak spustit Streamlit v Google Colabu.\")\n",
        "\n",
        "if st.button(\"Klikni\"):\n",
        "    st.success(\"Ahoj z Colabu!\")\n"
      ],
      "metadata": {
        "id": "P06cyqBzrNxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "!ngrok config add-authtoken \"2qIMKeezI8AM7htYMLgZ5CFZwIG_7W8qVRJ27QMuyoEGTReFs\"\n",
        "\n",
        "# Otevřeme ngrok tunel na port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Veřejná URL aplikace:\", public_url.public_url)\n",
        "\n",
        "# Funkce pro spuštění Streamlit v pozadí\n",
        "def run_streamlit():\n",
        "    # Spustíme streamlit na portu 8501\n",
        "    subprocess.call([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\",\n",
        "                     \"--server.enableCORS\", \"false\",\n",
        "                     \"--server.enableXsrfProtection\", \"false\"])\n",
        "\n",
        "# Spustíme Streamlit v separátním vlákně\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Volitelně lze přidat čekání, aby se stihlo vše spustit.\n",
        "time.sleep(3)\n",
        "print(\"Aplikace by měla být dostupná na adrese výše (ngrok).\")\n"
      ],
      "metadata": {
        "id": "ztuQEOOErRWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eEFh3KhJrNjx"
      }
    }
  ]
}